{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "27a0688c-634a-4e7f-96be-958fc2aba4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b0f299-3bc7-4947-9cbf-812780e8d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Load your DataFrame\n",
    "df = pd.read_csv('sentiment_analysis_results.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
    "# Separate positive and negative scores\n",
    "df['positive_score'] = df.apply(lambda row: row['Sentiment Score'] if row['Sentiment'] == 'POSITIVE' else 0, axis=1)\n",
    "df['negative_score'] = df.apply(lambda row: row['Sentiment Score'] if row['Sentiment'] == 'NEGATIVE' else 0, axis=1)\n",
    "\n",
    "\n",
    "label_columns = ['automotive', 'nonsense', 'energy', 'lifestyle', 'environment', 'travel', 'finance', 'science', 'politics']\n",
    "\n",
    "\n",
    "def set_top3_labels(row):\n",
    "    scores = row[label_columns].astype(float)  # Ensure all scores are float\n",
    "    if scores.isnull().all():\n",
    "        return row\n",
    "    top3_indices = scores.nlargest(3).index\n",
    "    for col in label_columns:\n",
    "        row[col] = 1 if col in top3_indices else 0\n",
    "    return row\n",
    "\n",
    "# Apply the function to each row\n",
    "df = df.apply(set_top3_labels, axis=1)\n",
    "\n",
    "# Group by 'Date' and calculate the average scores\n",
    "grouped_df = df.groupby('Date').agg({\n",
    "    'positive_score': 'mean',\n",
    "    'negative_score': 'mean',\n",
    "    'automotive': 'mean',\n",
    "    'nonsense': 'mean',\n",
    "    'energy': 'mean',\n",
    "    'lifestyle': 'mean',\n",
    "    'environment': 'mean',\n",
    "    'travel': 'mean',\n",
    "    'finance': 'mean',\n",
    "    'science': 'mean',\n",
    "    'politics': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns to match the required format\n",
    "grouped_df.columns = [\n",
    "    'Date', 'Average Positive Score', 'Average Negative Score',\n",
    "    'automotive', 'nonsense', 'energy', 'lifestyle', 'environment',\n",
    "    'travel', 'finance', 'science', 'politics'\n",
    "]\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(grouped_df)\n",
    "# Save the new DataFrame to a CSV file\n",
    "grouped_df.to_csv('data_prep1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e14097-b07f-49a8-af15-5c9dd884a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = 'TSLA'\n",
    "grouped_df = pd.read_csv('data_prep1.csv')\n",
    "grouped_df['Date'] = pd.to_datetime(grouped_df['Date']).dt.date\n",
    "df_stock_price = pd.read_csv('data\\stock_yfinance_data.csv')\n",
    "df_stock_price = df_stock_price[df_stock_price['Stock Name'] == stock_name ]\n",
    "df_stock_price['Date'] = pd.to_datetime(df_stock_price['Date']).dt.date\n",
    "def get_tech_ind(data):\n",
    "    data['MA7'] = data['Close'].rolling(window=7).mean()\n",
    "    data['MA20'] = data['Close'].rolling(window=20).mean()\n",
    "    data['MACD'] = data['Close'].ewm(span=26).mean() - data['Open'].ewm(span=12, adjust=False).mean()\n",
    "    data['20SD'] = data['Close'].rolling(window=20).std()\n",
    "    data['upper_band'] = data['MA20'] + (data['20SD'] * 2)\n",
    "    data['lower_band'] = data['MA20'] - (data['20SD'] * 2)\n",
    "    data['EMA'] = data['Close'].ewm(com=0.5).mean()\n",
    "    data['logmomentum'] = np.log(data['Close'] / data['Close'].shift(1))\n",
    "    return data\n",
    "label_df = get_tech_ind(df_stock_price)\n",
    "\n",
    "nearest_trading_days = label_df['Date'].sort_values().reset_index(drop=True)\n",
    "grouped_df['Nearest Trading Day'] = grouped_df['Date'].apply(lambda x: nearest_trading_days[nearest_trading_days <= x].max())\n",
    "\n",
    "# Merge tweet data onto stock price data using the 'Nearest Trading Day'\n",
    "final_df = pd.merge(grouped_df,label_df , left_on='Nearest Trading Day', right_on='Date', how='left')\n",
    "print(final_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d24a6d0-3182-4999-8e2c-404673406503",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "## Columns to shift\n",
    "final_df  = final_df.drop(columns = \"Stock Name\")\n",
    "price_columns = list(final_df.columns)\n",
    "price_columns.remove('Date_x')\n",
    "price_columns.remove('Date_y')\n",
    "price_columns.remove('Nearest Trading Day')\n",
    "print(price_columns)\n",
    "# Create new columns with the shifted values\n",
    "for col in price_columns:\n",
    "    final_df[f'Prev_{col}'] = final_df[col].shift(1)\n",
    "\n",
    "# Remove the first row which will have NaNs after shifting\n",
    "final_df = final_df.iloc[1:]\n",
    "\n",
    "# Drop the original price columns if needed\n",
    "price_columns.append('Date_x')\n",
    "price_columns.append('Nearest Trading Day')\n",
    "price_columns.remove('Close')\n",
    "final_df  = final_df.drop(columns=price_columns)\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(final_df)\n",
    "\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "final_df.to_csv('data_prep2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f58084-9ba8-4d2c-833a-172600e003c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df= pd.read_csv('data_prep2.csv')\n",
    "final_df= final_df.sort_values('Date_y')\n",
    "#test for non-tweets\n",
    "\"\"\"tmp= ['Prev_Average Positive Score','Prev_Average Negative Score','Prev_automotive','Prev_nonsense','Prev_energy','Prev_lifestyle','Prev_environment'\n",
    "      ,'Prev_travel','Prev_finance','Prev_science','Prev_politics']\n",
    "final_df  = final_df.drop(columns=tmp)\n",
    "\"\"\"\n",
    "split_ratio = 0.8\n",
    "split_point = int(len(final_df) * split_ratio)\n",
    "\n",
    "train = final_df[:split_point]\n",
    "test = final_df[split_point:]\n",
    "\n",
    "X_train = train.drop(['Date_y', 'Close'], axis=1)\n",
    "y_train = train['Close']\n",
    "X_test = test.drop(['Date_y', 'Close'], axis=1)\n",
    "y_test = test['Close']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321ec5d-5101-4d8a-b0f2-68980a3a5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Create DMatrix for XGBoost\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'subsample': [0.6, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error', cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Lowest RMSE found: \", (-grid_search.best_score_) ** 0.5)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31dc7a2-275f-453d-a306-7ee7452e1821",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test['Date_y'], y_test, label='Actual', color='blue')\n",
    "plt.plot(test['Date_y'], predictions, label='Predicted', color='red')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.title('Actual vs Predicted Close Prices')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e647ccd-c40c-402a-bf32-c4433aac19c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
